import os
import time

import openai
from rich.console import Console
from typing import Optional, List, Dict

from .base import LLM

CONSOLE = Console()


class OpenAILLM(LLM):
    """
    Implementation of the LLM interface for OpenAI models.
    """

    def __init__(
            self,
            api_key: Optional[str] = None,
            model_name: str = "gpt-3.5-turbo",
            max_tokens: int = 150,
            temperature: float = 0.7
    ):
        """
        Initializes the OpenAILLM instance.

        Args:
            api_key (Optional[str]): OpenAI API key. If None, will raise an error on usage.
            model_name (str): The name of the OpenAI model to use.
            max_tokens (int): Maximum number of tokens for the response.
            temperature (float): Sampling temperature for randomness in responses.
        """
        if not api_key:
            CONSOLE.print("[bold red]Warning:[/bold red] API key is not provided.")
        self.api_key = api_key
        openai.api_key = self.api_key
        self.model_name = model_name
        self.max_tokens = max_tokens
        self.temperature = temperature

    def generate(self, prompt: str) -> str:
        """
        Generate a bash command for awslogs based on user requirements.

        Args:
            prompt (str): The user input to generate a command.

        Returns:
            str: The bash command generated by the model.
        """
        try:
            response = openai.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": (
                        "You are an intelligent assistant specialized in creating bash commands for querying AWS logs using the awslogs CLI tool. "
                        "You provide efficient, concise, and accurate commands based on user requirements. "
                        "Only return the raw command in plain text without any formatting, explanations, or code block markers."
                    )},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=self.max_tokens,
                temperature=self.temperature,
            )
            return response.choices[0].message.content.strip()
        except openai.AuthenticationError:
            CONSOLE.print("[bold red]Authentication Error:[/bold red] Invalid API key.")
            raise
        except openai.OpenAIError as e:
            CONSOLE.print(f"[bold red]OpenAI API Error:[/bold red] {e}")
            raise
        except Exception as e:
            CONSOLE.print(f"[bold red]Unexpected Error:[/bold red] {e}")
            raise

    def is_ready(self) -> bool:
        """
        Checks if the LLM is ready to generate responses.

        Returns:
            bool: True if the API key and model name are set, False otherwise.
        """
        return bool(self.api_key and self.model_name)

    def fine_tune(self, dataset: List[Dict[str, str]]):
        """
        Fine-tunes the OpenAI model with the given dataset.

        Args:
            dataset (List[Dict[str, str]]): List of dictionaries with 'prompt' and 'completion' keys.
        """
        try:
            # Save dataset to JSONL
            jsonl_file = "fine_tune_data.jsonl"
            with open(jsonl_file, "w") as f:
                for entry in dataset:
                    print(entry)
                    f.write(f"{entry}\n")

            CONSOLE.print(f"[bold blue]Uploading fine-tuning dataset to OpenAI...[/bold blue]")

            # Upload file to OpenAI
            file_upload = openai.files.create(
                file=open(jsonl_file, "rb"),
                purpose="fine-tune"
            )
            file_id = file_upload.id
            CONSOLE.print(f"[bold green]Dataset uploaded successfully. File ID: {file_id}[/bold green]")

            # Start fine-tuning
            CONSOLE.print(f"[bold blue]Starting fine-tuning with base model: {self.model_name}...[/bold blue]")
            fine_tune_job = openai.fine_tuning.jobs.create(
                training_file=file_id,
                model=self.model_name
            )
            fine_tune_id = fine_tune_job.id
            CONSOLE.print(f"[bold green]Fine-tuning started. Job ID: {fine_tune_id}[/bold green]")

            # Monitor fine-tuning status
            CONSOLE.print("[bold yellow]Monitoring fine-tuning job status. This may take some time...[/bold yellow]")
            while True:
                job_status = openai.fine_tuning.jobs.retrieve(fine_tune_id)
                status = job_status.status
                if status == "succeeded":
                    CONSOLE.print("[bold green]Fine-tuning completed successfully![/bold green]")
                    CONSOLE.print(f"[bold blue]Fine-tuned model: {job_status['fine_tuned_model']}[/bold blue]")
                    break
                elif status == "failed":
                    CONSOLE.print("[bold red]Fine-tuning failed. Check OpenAI logs for more details.[/bold red]")
                    break
                else:
                    CONSOLE.print(f"[bold blue]Status: {status}[/bold blue]")
                    time.sleep(1)
        except openai.OpenAIError as e:
            CONSOLE.print(f"[bold red]OpenAI API Error:[/bold red] {e}")
            raise
        except Exception as e:
            CONSOLE.print(f"[bold red]Unexpected Error:[/bold red] {e}")
            raise
        finally:
            # Clean up the temporary dataset file
            if os.path.exists(jsonl_file):
                os.remove(jsonl_file)
                CONSOLE.print(f"[bold green]Temporary dataset file {jsonl_file} deleted.[/bold green]")

    def set_parameters(self, *, max_tokens: Optional[int] = None, temperature: Optional[float] = None):
        """
        Updates the model parameters dynamically.

        Args:
            max_tokens (Optional[int]): Maximum number of tokens for the response.
            temperature (Optional[float]): Sampling temperature for randomness in responses.
        """
        if max_tokens is not None:
            self.max_tokens = max_tokens
        if temperature is not None:
            self.temperature = temperature
